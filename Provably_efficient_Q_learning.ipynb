{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3RPRx9pDUsY",
        "outputId": "49933c63-faee-4a8d-c425-94f5eff1db89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "OhatI4mwP7Ml",
        "outputId": "8b75a900-f0c0-433f-b89d-cc7e44d54490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import TransformReward\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEiqoo6R9e6z",
        "outputId": "b069906e-1423-41d5-d0d4-54fdea84a18b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tBLNwQNCsYn"
      },
      "outputs": [],
      "source": [
        "class DrunkManEnvBase(gym.Env):\n",
        "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
        "    metadata = {'render.modes': ['console']}\n",
        "\n",
        "    def __init__(self, p=0.7, min_position=0, max_position=10, goal_position=10, penalty_case=False, H=None, discounted_reward = False, circle_mdp=False, h =0):\n",
        "        super(DrunkManEnvBase, self).__init__()\n",
        "\n",
        "        # Action space: 0 (backward), 1 (forward)\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Observation space: Position along the line\n",
        "        self.observation_space = spaces.Discrete(max_position+1)\n",
        "\n",
        "        # Environment parameters\n",
        "        self.p = p\n",
        "        self.min_position = min_position\n",
        "        self.max_position = max_position\n",
        "        self.goal_position = goal_position\n",
        "        self.state = None\n",
        "        self.discounted_reward = discounted_reward\n",
        "        self.penalty_case = penalty_case\n",
        "        self.H = H\n",
        "        self.circle_mdp = circle_mdp\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.min_position\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action, h=0):\n",
        "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
        "        done = bool(self.state >= self.goal_position)\n",
        "        # Determine movement based on the probability\n",
        "        if np.random.rand() < self.p:\n",
        "            movement = 1 if action == 1 else -1\n",
        "        else:\n",
        "            movement = -1 if action == 1 else 1\n",
        "\n",
        "\n",
        "        if self.circle_mdp and done:\n",
        "          done = 0\n",
        "          self.reset()\n",
        "        else:\n",
        "        # Update the position\n",
        "          if not done:\n",
        "            self.state += movement\n",
        "            self.state = np.clip(self.state, self.min_position, self.max_position)\n",
        "          if done:\n",
        "            self.state = self.state\n",
        "          done = bool(self.state >= self.goal_position)\n",
        "        # Check if the goal is reached\n",
        "\n",
        "\n",
        "        # Assign reward\n",
        "        if self.penalty_case:\n",
        "            reward = self.H if done else -1\n",
        "        if self.discounted_reward:\n",
        "            reward = (self.H - h)/self.H if done else 0\n",
        "        elif self.circle_mdp:\n",
        "            reward = (self.max_position+1)/self.H if done else 0\n",
        "        else:\n",
        "            reward = 1/self.H if done else 0\n",
        "\n",
        "        info = {}\n",
        "\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self, mode='console'):\n",
        "        if mode != 'console':\n",
        "            raise NotImplementedError()\n",
        "        print(f\"Position: {self.state}\")\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KptgrI3BDYd6"
      },
      "outputs": [],
      "source": [
        "class DrunkManEnv(object):\n",
        "    def __init__(self, drift_probability=0.85, space_length=7, H=15,env_params={}):\n",
        "        self.base_env = DrunkManEnvBase(p=env_params.get('drift_probability',drift_probability),\n",
        "                                        max_position=env_params.get('space_length',space_length),\n",
        "                                        goal_position=env_params.get('space_length',space_length),\n",
        "                                        penalty_case=env_params.get('penalty_case',False),\n",
        "                                        H=H,\n",
        "                                        discounted_reward=env_params.get('discounted_reward',False),\n",
        "                                        circle_mdp = env_params.get('circle_mdp',False))\n",
        "        self.action_space = self.base_env.action_space\n",
        "        self.A = self.action_space.n\n",
        "        self.state_space = self.base_env.observation_space\n",
        "        self.S = self.state_space.n\n",
        "        self.H = H\n",
        "        self.p = env_params.get('drift_probability',drift_probability)\n",
        "        self.env_params = env_params\n",
        "        self.pi_star = np.ones([self.H, self.S])\n",
        "        self.absorbing_states = [self.S - 1]\n",
        "\n",
        "    def compute_transition(self):\n",
        "        self.P = {a:np.zeros([self.S,self.S]) for a in range(self.A)}\n",
        "        self.R = {a:0 for a in range(self.A)}\n",
        "        action_map = {0:'left',1:'right'}\n",
        "        def action_idx_map(a, idx, max_idx):\n",
        "            next_state_left = max(0, idx-1)\n",
        "            next_state_right = min(idx+1, max_idx)\n",
        "            if a == 0:\n",
        "                next_state_list = [next_state_left, next_state_right]\n",
        "            else:\n",
        "                next_state_list = [next_state_right, next_state_left]\n",
        "            output_dict = {next_state_list[0]:self.p, next_state_list[1]:1-self.p}\n",
        "            return output_dict\n",
        "\n",
        "        for idx in range(self.S):\n",
        "            for a in range(self.A):\n",
        "                P_trans = self.P[a].copy()\n",
        "                if idx==self.S-1:\n",
        "                  if not self.env_params.get('circle_mdp',False):\n",
        "                    P_trans[idx, idx] = 1\n",
        "                  else:\n",
        "                    P_trans[idx, 0] = 1\n",
        "                else:\n",
        "                    next_state_prob_dict = action_idx_map(a,idx,self.S)\n",
        "                    for s,p in next_state_prob_dict.items():\n",
        "                        P_trans[idx, s] = p\n",
        "                self.P[a] = P_trans\n",
        "\n",
        "    def value_iteration(self):\n",
        "        self.V_prime = np.zeros([self.H + 1, self.S])\n",
        "        self.Q_prime = np.zeros([self.H, self.S, self.A])\n",
        "        for h in range(self.H - 1, -1, -1):\n",
        "            for s in range(self.S):\n",
        "                for a in range(self.A):\n",
        "                    if self.env_params.get('penalty_case',False):\n",
        "                        self.Q_prime[h][s][a] = np.sum(self.P[a][s] * (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (self.H * int(s != self.S - 1)) - 1\n",
        "                    elif self.env_params.get('discounted_reward',False):\n",
        "                        self.Q_prime[h][s][a] = self.env_params.get('gamma',0.99) * np.sum(self.P[a][s] * (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * ((self.H - h)/self.H) * int(s != self.S - 1)\n",
        "                    elif self.env_params.get('circle_mdp',False):\n",
        "                        self.Q_prime[h][s][a] = (self.P[a][s] @ (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (self.S/self.H)\n",
        "                    else:\n",
        "                        self.Q_prime[h][s][a] = (self.P[a][s] @ (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (1/self.H)\n",
        "\n",
        "            self.V_prime[h] = self.Q_prime[h].max(axis=1)\n",
        "\n",
        "    def policy_evaluation(self,policy):\n",
        "        V_policy = np.zeros([self.H + 1, self.S])\n",
        "        Q_policy = np.zeros([self.H, self.S, self.A])\n",
        "        for h in range(self.H - 1, -1, -1):\n",
        "            for s in range(self.S):\n",
        "                for a in range(self.A):\n",
        "                    if self.env_params.get('penalty_case',False):\n",
        "                        self.Q_prime[h][s][a] = np.sum(self.P[a][s] * (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (self.H * int(s != self.S - 1)) - 1\n",
        "                    elif self.env_params.get('discounted_reward',False):\n",
        "                        self.Q_prime[h][s][a] = self.env_params.get('gamma',0.99) * np.sum(self.P[a][s] * (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (1 * int(s != self.S - 1))\n",
        "                    else:\n",
        "                        self.Q_prime[h][s][a] = np.sum(self.P[a][s] * (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (1 * int(s != self.S - 1))\n",
        "                V_policy[h][s] = Q_policy[h][s][policy[h,s]]\n",
        "\n",
        "        return V_policy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o6RysI6PTmG"
      },
      "outputs": [],
      "source": [
        "class FrozenLakeEnv(object):\n",
        "    def __init__(self, map_name=\"4x4\", is_slippery=True, simple_example=True, H=25,env_params={}):\n",
        "        is_slippery = env_params.get('is_slippery', is_slippery)\n",
        "        if env_params.get('simple_example',simple_example):\n",
        "            self.base_env = gym.make('FrozenLake-v1', map_name=map_name, desc=[\"SFFF\", \"FFFF\", \"FFFF\", \"FFFG\"], is_slippery=is_slippery)\n",
        "        else:\n",
        "            self.base_env = gym.make('FrozenLake-v1', desc = env_params.get('desc_map',[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]), is_slippery=is_slippery)\n",
        "        self.action_space = self.base_env.action_space\n",
        "        self.A = self.action_space.n\n",
        "        self.state_space = self.base_env.observation_space\n",
        "        self.S = self.state_space.n\n",
        "        self.is_slippery = is_slippery\n",
        "        self.H = H\n",
        "        self.env_params = env_params\n",
        "        max_row_idx = len(self.base_env.desc)\n",
        "        max_col_idx = len(self.base_env.desc[0])\n",
        "        self.absorbing_states = [row_idx*max_col_idx + col_idx for row_idx in range(len(self.base_env.desc)) for col_idx in range(len(self.base_env.desc[row_idx])) if self.base_env.desc[row_idx][col_idx] in [b'H', b'G']]\n",
        "        if env_params.get('circle_mdp',False):\n",
        "            self.base_env = TransformReward(self.base_env, lambda x: 6*x/self.H)\n",
        "        elif not env_params.get('discounted_reward',False) and self.env_params.get('gamma') == 1:\n",
        "            self.base_env = TransformReward(self.base_env, lambda x: x/self.H)\n",
        "\n",
        "    def compute_transition(self):\n",
        "        self.P = {a:np.zeros([self.S,self.S]) for a in range(self.A)}\n",
        "        self.R = {a:0 for a in range(self.A)}\n",
        "        max_row_idx = len(self.base_env.desc)\n",
        "        max_col_idx = len(self.base_env.desc[0])\n",
        "        action_map = {0:'left',1:'down',2:'right',3:'up'}\n",
        "        def action_idx_map(a, row_idx, col_idx, is_slippery=self.is_slippery):\n",
        "            next_state_left = row_idx * max_col_idx + max(0, col_idx - 1)\n",
        "            next_state_down = min(max_row_idx-1, row_idx + 1) * max_col_idx + col_idx\n",
        "            next_state_right = row_idx * max_col_idx + min(max_col_idx-1, col_idx + 1)\n",
        "            next_state_up = max(0, row_idx - 1) * max_col_idx + col_idx\n",
        "            next_state_list = [next_state_left, next_state_down,next_state_right,next_state_up]\n",
        "            perpendicular_action_dict = {0:[1,3],1:[0,2],2:[1,3],3:[0,2]}\n",
        "            if not is_slippery:\n",
        "                output_dict = {next_state_list[a]:1}\n",
        "            else:\n",
        "                output_dict = {}\n",
        "                for next_state in [next_state_list[a], next_state_list[perpendicular_action_dict[a][0]], next_state_list[perpendicular_action_dict[a][1]]]:\n",
        "                    if next_state not in output_dict:\n",
        "                        output_dict[next_state] = 1/3\n",
        "                    else:\n",
        "                        output_dict[next_state] = output_dict[next_state] + 1/3\n",
        "            return output_dict\n",
        "\n",
        "        for row_idx in range(len(self.base_env.desc)):\n",
        "            for col_idx in range(len(self.base_env.desc[row_idx])):\n",
        "                for a in range(self.A):\n",
        "                    P_trans = self.P[a].copy()\n",
        "                    if self.base_env.desc[row_idx][col_idx] in [b'H', b'G'] and not self.env_params.get('circle_mdp', False):\n",
        "                        P_trans[row_idx*max_col_idx + col_idx, row_idx*max_col_idx + col_idx] = 1\n",
        "                    elif self.base_env.desc[row_idx][col_idx] in [b'H', b'G'] and self.env_params.get('circle_mdp', False):\n",
        "                        P_trans[row_idx*max_col_idx + col_idx, 0] = 1\n",
        "                    else:\n",
        "                        next_state_prob_dict = action_idx_map(a, row_idx, col_idx)\n",
        "                        for s,p in next_state_prob_dict.items():\n",
        "                            P_trans[row_idx*max_col_idx + col_idx, s] = p\n",
        "                    self.P[a] = P_trans\n",
        "\n",
        "    def value_iteration(self):\n",
        "        self.V_prime = np.zeros([self.H + 1, self.S])\n",
        "        self.Q_prime = np.zeros([self.H, self.S, self.A])\n",
        "\n",
        "        for h in range(self.H - 1, -1, -1):\n",
        "            for s in range(self.S):\n",
        "                for a in range(self.A):\n",
        "                    if self.env_params.get('penalty_case',False):\n",
        "                        self.Q_prime[h][s][a] = np.sum(self.P[a][s] * (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (self.H * int(s != self.S - 1)) - 1\n",
        "                    elif self.env_params.get('discounted_reward',False):\n",
        "                        self.Q_prime[h][s][a] = self.env_params.get('gamma',0.99) * np.sum(self.P[a][s] * (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * ((self.H - h)/self.H) * int(s != self.S - 1)\n",
        "                    elif self.env_params.get('circle_mdp',False):\n",
        "                        self.Q_prime[h][s][a] = (self.P[a][s] @ (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (6/self.H)\n",
        "                    else:\n",
        "                        self.Q_prime[h][s][a] = (self.P[a][s] @ (self.V_prime[h + 1])) + self.P[a][s][\n",
        "                            self.S - 1] * (1/self.H)\n",
        "\n",
        "            self.V_prime[h] = self.Q_prime[h].max(axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    def policy_evaluation(self,policy):\n",
        "        V_policy = np.zeros([self.H + 1, self.S])\n",
        "        Q_policy = np.zeros([self.H, self.S, self.A])\n",
        "        for h in range(self.H - 1, -1, -1):\n",
        "            for s in range(self.S):\n",
        "                for a in range(self.A):\n",
        "                    if self.env_params.get('discounted_reward',False):\n",
        "                        Q_policy[h][s][a] = self.env_params.get('gamma',0.99) * np.sum(self.P[a][s] * (V_policy[h + 1])) + self.P[a][s][\n",
        "                        self.S - 1] * int(s != self.S - 1)\n",
        "                    else:\n",
        "                        Q_policy[h][s][a] = np.sum(self.P[a][s] * (V_policy[h + 1])) + self.P[a][s][\n",
        "                        self.S - 1] * int(s != self.S - 1)\n",
        "\n",
        "                V_policy[h][s] = Q_policy[h][s][policy[h,s]]\n",
        "\n",
        "        return V_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWiHX5m51PTU"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "Env_mapping = {\n",
        "    'FrozenLake':FrozenLakeEnv,\n",
        "    'DrunkMan':DrunkManEnv\n",
        "}\n",
        "class ProvablyEfficientQLearning(object):\n",
        "    def __init__(self, bonus_type='Hoeffding', H=25, K=20000, c=0.01, env_type = 'FrozenLake', env_params={}):\n",
        "        self.Env = Env_mapping[env_type](H=H, env_params=env_params)\n",
        "        self.env_type = env_type\n",
        "        self.env = self.Env.base_env\n",
        "        self.A = self.Env.A\n",
        "        self.S = self.Env.S\n",
        "        self.H = self.Env.H\n",
        "        self.K = K\n",
        "        self.T = self.K * self.H\n",
        "        self.max_V = self.H if env_params.get('penalty_case',False) else 1\n",
        "        self.Q = np.ones([self.H, self.S, self.A]) * self.max_V\n",
        "        self.V = np.ones([self.H + 1, self.S]) * self.max_V\n",
        "        self.V[self.H] = np.zeros(self.S) # This one is the must\n",
        "        if env_params.get('discounted_reward', False):\n",
        "            for s in self.Env.absorbing_states:\n",
        "                self.V[:, int(s)] = np.zeros(self.H + 1)\n",
        "        self.N = np.zeros([self.H, self.S, self.A])\n",
        "        self.bonus_type = bonus_type\n",
        "        self.c = c # need more tuning\n",
        "        self.p = .05 # need more tuning\n",
        "        self.tau = np.log(self.S * self.A  *self.T / self.p) #np.log(self.S * self.A  *self.K / self.p) #\n",
        "        self.logging_Q = {}\n",
        "        self.gamma = 1 if not env_params.get('discounted_reward',False) else env_params.get('gamma',0.99)\n",
        "        self.env_params = env_params\n",
        "        self.no_early_stopping = (env_params.get('circle_mdp', False) or (not env_params.get('discounted_reward', False)) )\n",
        "        if self.bonus_type == 'Bernstein':\n",
        "            self.mu = np.zeros([self.H, self.S, self.A])\n",
        "            self.sigma = np.zeros([self.H, self.S, self.A])\n",
        "            self.betaold = np.zeros([self.H, self.S, self.A])\n",
        "            self.betanew = np.zeros([self.H, self.S, self.A])\n",
        "            self.c1 = self.c2 = self.c\n",
        "\n",
        "    def learnRate(self, t):\n",
        "        return (self.H + 1)/ (self.H + t)\n",
        "\n",
        "    def Bonusterm(self, t, h,s,a,s1):\n",
        "        if self.bonus_type == 'Hoeffding':\n",
        "            return self.c*np.sqrt(  (self.max_V)**2 * self.tau/ t) # self.c*np.sqrt(  self.H * (self.max_V)**2 * self.tau/ t)\n",
        "        elif self.bonus_type == 'Bernstein':\n",
        "            self.mu[h,s,a] = self.mu[h,s,a] + self.V[h+1,s1]\n",
        "            self.sigma[h,s,a] = self.sigma[h,s,a] + (self.V[h+1,s1])**2\n",
        "            beta_term_111 = (self.sigma[h,s,a] - (self.mu[h,s,a])**2)/t + self.H\n",
        "            beta_term_11 = np.sqrt(self.H * beta_term_111 * self.tau / t)\n",
        "            beta_term_12 = np.sqrt((self.H)**7 * self.S * self.A) * self.tau / t\n",
        "            beta_term_1 = self.c1 * (beta_term_11 + beta_term_12)\n",
        "            beta_term_2 = self.c2 * np.sqrt( (self.H)**3 * self.tau / t)\n",
        "            beta = min(beta_term_1, beta_term_2)\n",
        "            self.betaold[h,s,a] = self.betanew[h,s,a]\n",
        "            self.betanew[h,s,a] = beta\n",
        "            alpha = self.learnRate(t)\n",
        "            bonus = (beta - (1-alpha)*self.betaold[h,s,a]) / (2*alpha)\n",
        "            return bonus\n",
        "\n",
        "    def train(self):\n",
        "        self.rList = []\n",
        "        # for h in range(self.H):\n",
        "        #     for s in range(self.S):\n",
        "        #       for a in range(self.A):\n",
        "        #         self.Q[h,s,a] += self.Bonusterm(1, h, s, a, s)\n",
        "        self.rList_stepwise = []\n",
        "        for k in tqdm(range(self.K)):\n",
        "            s = self.env.reset()\n",
        "            d = False\n",
        "            r_k = 0\n",
        "            trajectory = {}\n",
        "            for h in range(self.H):\n",
        "                a = np.random.choice(np.where(self.Q[h,s,:] == self.Q[h,s,:].max())[0])\n",
        "                self.N[h,s,a] += 1\n",
        "                t = self.N[h,s,a]\n",
        "                s1, r, d, _ = self.env.step(a)\n",
        "                if self.env_type == 'FrozenLake' and d and self.env_params.get('circle_mdp', False) and s == self.S-1:\n",
        "                  s1 = self.env.reset()\n",
        "                if self.env_type == 'FrozenLake'  and d and not self.env_params.get('discounted_reward', False) and  s == s1 == self.S-1:\n",
        "                  r = self.rList_stepwise[-1]\n",
        "                if d and self.env_params.get('discounted_reward', False) and s1 == self.S-1:\n",
        "                  r = (self.H - h) / self.H\n",
        "                trajectory[h] = (s,a,s1,r,d)\n",
        "                r_k += r*(self.gamma**h)\n",
        "                self.rList_stepwise.append(r*(self.gamma**h))\n",
        "                if d and not self.no_early_stopping:\n",
        "                  break\n",
        "                s = s1\n",
        "\n",
        "\n",
        "            if k%10000 == 0:\n",
        "                print([trajectory[h][0] for h in range(self.H) if h in trajectory])\n",
        "            for h in range(self.H - 1, -1, -1):\n",
        "                if h in trajectory:\n",
        "                  s,a,s1,r,d = trajectory[h]\n",
        "                  t = self.N[h,s,a]\n",
        "                  b = self.Bonusterm(t, h,s,a,s1)\n",
        "                  # print(b)\n",
        "                  lr = self.learnRate(t)\n",
        "                  self.Q[h,s,a] = (1-lr) * self.Q[h,s,a] + lr*(r + self.gamma * self.V[h+1,s1]  +b)\n",
        "                  self.V[h,s] = min(np.max(self.Q[h,s,:]),self.max_V)\n",
        "            # self.logging_Q[k] = self.Q.copy()\n",
        "            self.rList.append(r_k)\n",
        "        self.trajectory = trajectory\n",
        "\n",
        "    def stepwise_regret(self):\n",
        "        self.regret_step_wise = []\n",
        "        self.Env.compute_transition()\n",
        "        self.Env.value_iteration()\n",
        "        self.pi_star = self.Env.Q_prime.argmax(axis=2)\n",
        "        total_steps = len(self.rList_stepwise)\n",
        "        current_steps = 0\n",
        "        current_cum_regret = 0\n",
        "        while current_steps < total_steps:\n",
        "            s = self.env.reset()\n",
        "            d = False\n",
        "            for h in range(self.H):\n",
        "                a = int(self.pi_star[h][s])\n",
        "                s1, r, d, _ = self.env.step(a)\n",
        "                if d and self.env_params.get('discounted_reward', False) and s1 == self.S-1:\n",
        "                    r = (self.H - h) / self.H\n",
        "                current_cum_regret += r*(self.gamma**h) - self.rList_stepwise[current_steps]\n",
        "                if current_steps % self.H == 0:\n",
        "                    self.regret_step_wise.append(current_cum_regret)\n",
        "\n",
        "                s = s1\n",
        "                current_steps += 1\n",
        "                if d and not self.no_early_stopping:\n",
        "                    break\n",
        "                if current_steps >= total_steps:\n",
        "                    break\n",
        "    def regret_compute(self):\n",
        "        regret = []\n",
        "        self.Env.compute_transition()\n",
        "        self.Env.value_iteration()\n",
        "        V_prime = self.Env.V_prime\n",
        "        for k in tqdm(range(self.K)):\n",
        "            Q_k = self.logging_Q[k]\n",
        "            policy_k = Q_k.argmax(axis=2)\n",
        "            eval = self.Env.policy_evaluation(policy_k)\n",
        "            if regret:\n",
        "                regret.append(regret[k-1]+ (V_prime[0][0] - eval[0][0]))\n",
        "            else:\n",
        "                regret.append( (V_prime[0][0] - eval[0][0]))\n",
        "\n",
        "        return regret, eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPQWPjOjfTd5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8xgkbGCqAYE"
      },
      "outputs": [],
      "source": [
        "class PosteriorSamplingQLearning(object):\n",
        "    def __init__(self, H=100, K=20000, c=2, zero_var_start=False, env_type='FrozenLake',env_params={}):\n",
        "        self.Env = Env_mapping[env_type](H=H,env_params=env_params)\n",
        "        self.env = self.Env.base_env\n",
        "        self.A = self.Env.A\n",
        "        self.S = self.Env.S\n",
        "        self.H = self.Env.H\n",
        "        self.K = K\n",
        "        self.max_V = self.H if env_params.get('penalty_case',False) else 1\n",
        "\n",
        "        # predefine the maximum V you can get here\n",
        "        self.T = self.K * self.H\n",
        "        self.Q = np.ones([self.H, self.S, self.A]) * self.max_V #initialize with that maximum\n",
        "        # self.Q[self.H] = np.zeros([self.S,self.A])\n",
        "        self.V = np.ones([self.H + 1, self.S]) * self.max_V #initialize with that maximum\n",
        "        self.V[self.H] = np.zeros(self.S)\n",
        "        if env_params.get('discounted_reward', False):\n",
        "            for s in self.Env.absorbing_states:\n",
        "                self.Q[:, int(s), :] = np.zeros([self.H, self.A])\n",
        "                self.V[:, int(s)] = np.zeros(self.H + 1)\n",
        "        self.V_bar = self.V.copy()\n",
        "        self.N = np.zeros([self.H, self.S, self.A])\n",
        "        self.N_last = np.zeros(self.S)\n",
        "        ## todo the parameter setting for b and c\n",
        "\n",
        "        self.c = c # the multiplicative a for variance\n",
        "        self.b = 1\n",
        "        self.p = .05 # the \\delta in script\n",
        "        self.gamma = 1 if not env_params.get('discounted_reward',False) else env_params.get('gamma',0.99)\n",
        "        self.J = self.b * np.log( self.S * self.A * self.H * self.T / self.p)\n",
        "        self.logging_Q = {}\n",
        "        self.zero_var_start = zero_var_start\n",
        "        self.no_early_stopping = (env_params.get('circle_mdp', False) or (not env_params.get('discounted_reward', False)) )\n",
        "        self.env_params = env_params\n",
        "        self.env_type = env_type\n",
        "\n",
        "    def learnRate(self, t):\n",
        "        return (self.H + 1) / (self.H + t)\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.rList = []\n",
        "        self.rList_stepwise = []\n",
        "\n",
        "        sample_size = int(self.J)\n",
        "        # for h in range(self.H):\n",
        "        #     for s in range(self.S):\n",
        "        #       for a in range(self.A):\n",
        "        #         self.Q[h,s,a] += self.Bonusterm(1, h, s, a, s)\n",
        "        for k in tqdm(range(self.K)):\n",
        "            s = self.env.reset()\n",
        "            d = False\n",
        "            rEpisode = 0\n",
        "            trajectory = {}\n",
        "            sample_trajectory = {}\n",
        "            for h in range(self.H):\n",
        "                # compute your variance with H*(V_max)^2 instead of H^3\n",
        "                var = self.c*( (self.max_V ** 2)) / np.maximum(np.ones(self.A), self.N[h,s,:])\n",
        "                Q_sample = self.Q[h,s,:] + np.random.randn(self.A) * np.sqrt(var)\n",
        "                a = np.argmax(Q_sample)\n",
        "                s1, r, d, _ = self.env.step(a)\n",
        "                if self.env_type == 'FrozenLake' and d and self.env_params.get('circle_mdp', False) and s == self.S-1:\n",
        "                  s1 = self.env.reset()\n",
        "                if self.env_type == 'FrozenLake' and d and not self.env_params.get('discounted_reward', False) and s == s1 == self.S-1:\n",
        "                  r = self.rList_stepwise[-1]\n",
        "                if d and self.env_params.get('discounted_reward', False) and s1 == self.S-1:\n",
        "                  r = (self.H - h) / self.H\n",
        "                rEpisode += r*(self.gamma**h)\n",
        "                self.rList_stepwise.append(r*(self.gamma**h))\n",
        "                trajectory[h] = (s,a,s1,r,d)\n",
        "                sample_trajectory[h,s] = Q_sample\n",
        "                self.N[h, s, a] += 1\n",
        "                s = s1\n",
        "\n",
        "\n",
        "                if d and not self.no_early_stopping:\n",
        "                    break\n",
        "\n",
        "            if k%10000 == 0:\n",
        "                print([trajectory[h][0] for h in range(self.H) if h in trajectory])\n",
        "            self.N_last[s] += 1\n",
        "            logging_Q = self.Q.copy()\n",
        "            for h in range(self.H-1, -1, -1):\n",
        "                if h in trajectory:\n",
        "                    s,a,s1,r,d = trajectory[h]\n",
        "                    t = self.N[h, s, a]\n",
        "                    lr = self.learnRate(t)\n",
        "                    # compute your variance with H*(V_max)^2 instead of H^3\n",
        "                    if h < self.H - 1 and (self.no_early_stopping or s1 not in self.Env.absorbing_states):\n",
        "                        var1 = self.c*(  (self.max_V ** 2)) / np.maximum(np.ones(self.A), self.N[h+1,s1,:])\n",
        "                        q_sample = self.Q[h+1,s1,:].reshape(self.A, 1) + np.random.randn(self.A, sample_size) * np.sqrt(var1.reshape(self.A, 1))\n",
        "                        V_next = min(self.max_V, np.max(q_sample))\n",
        "                    else:\n",
        "                        # var1 = self.c*(  (self.max_V ** 2)) / np.maximum(np.ones(self.A), self.N_last[s1])\n",
        "                        V_next = 0\n",
        "\n",
        "                    # update your Q with r + V_next only if 1. not terminated 2. current h<=H, else update it with r only\n",
        "                    self.Q[h, s, a] = (1 - lr) * self.Q[h, s, a] + lr * (r + self.gamma * V_next)\n",
        "                    logging_Q[h] = self.Q[h]\n",
        "                    logging_Q[h][s] = sample_trajectory[h,s]\n",
        "\n",
        "                    self.V[h, s] = min(np.max(self.Q[h, s, :]), self.max_V)\n",
        "\n",
        "\n",
        "            # self.logging_Q[k] = logging_Q\n",
        "            self.rList.append(rEpisode)\n",
        "        self.trajectory = trajectory\n",
        "\n",
        "    def stepwise_regret(self):\n",
        "        self.regret_step_wise = []\n",
        "        self.Env.compute_transition()\n",
        "        self.Env.value_iteration()\n",
        "        self.pi_star = self.Env.Q_prime.argmax(axis=2)\n",
        "        total_steps = len(self.rList_stepwise)\n",
        "        current_steps = 0\n",
        "        current_cum_regret = 0\n",
        "        while current_steps < total_steps:\n",
        "            s = self.env.reset()\n",
        "            # d = False\n",
        "            for h in range(self.H):\n",
        "                a = int(self.pi_star[h][s])\n",
        "                s1, r, d, _ = self.env.step(a)\n",
        "                if d and self.env_params.get('discounted_reward', False) and s1 == self.S-1:\n",
        "                    r = (self.H - h) / self.H\n",
        "                current_cum_regret += r*(self.gamma**h) - self.rList_stepwise[current_steps]\n",
        "                if current_steps % self.H == 0:\n",
        "                    self.regret_step_wise.append(current_cum_regret)\n",
        "                s = s1\n",
        "                current_steps += 1\n",
        "                if d and not self.no_early_stopping:\n",
        "                    break\n",
        "                if current_steps >= total_steps:\n",
        "                    break\n",
        "\n",
        "    def regret_compute(self):\n",
        "        regret = []\n",
        "        self.Env.compute_transition()\n",
        "        self.Env.value_iteration()\n",
        "        V_prime = self.Env.V_prime\n",
        "        for k in tqdm(range(self.K)):\n",
        "            Q_k = self.logging_Q[k]\n",
        "            policy_k = Q_k.argmax(axis=2)\n",
        "            eval = self.Env.policy_evaluation(policy_k)\n",
        "            if regret:\n",
        "                regret.append(regret[k-1]+ (V_prime[0][0] - eval[0][0]))\n",
        "            else:\n",
        "                regret.append( (V_prime[0][0] - eval[0][0]))\n",
        "\n",
        "        return regret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui5yx1xoOcY5"
      },
      "outputs": [],
      "source": [
        "class RLSVI(object):\n",
        "    def __init__(self, H=100, K=20000, c=2, zero_var_start=False, env_type='FrozenLake',env_params={}):\n",
        "        self.Env = Env_mapping[env_type](H=H,env_params=env_params)\n",
        "        self.env = self.Env.base_env\n",
        "        self.A = self.Env.A\n",
        "        self.S = self.Env.S\n",
        "        self.H = self.Env.H\n",
        "        self.K = K\n",
        "        self.max_V = self.H if env_params.get('penalty_case',False) else 1\n",
        "\n",
        "        # predefine the maximum V you can get here\n",
        "        self.T = self.K * self.H\n",
        "        self.beta = {k: .5 * c * (self.max_V**2) *np.log(2 * self.H * self.S * self.A * (k+1)) for k in range(self.K)} #* self.H\n",
        "        self.Q = np.random.randn(self.H + 1, self.S, self.A) * np.sqrt(self.beta[0]) #initialize with that maximum\n",
        "        self.Q[self.H] = np.zeros([self.S,self.A])\n",
        "        self.V = np.clip(self.Q.max(axis=2),0,self.max_V)\n",
        "        if env_params.get('discounted_reward', False):\n",
        "            for s in self.Env.absorbing_states:\n",
        "                self.Q[:, int(s),:] = np.zeros([self.H + 1, self.A])\n",
        "                self.V[:, int(s)] = np.zeros(self.H+1)\n",
        "        self.V_bar = self.V.copy()\n",
        "        self.N = np.zeros([self.H, self.S, self.A])\n",
        "        self.D = np.zeros([self.H+1, self.S, self.A ,self.S+1])\n",
        "        self.c = c\n",
        "        self.gamma = 1 if not env_params.get('discounted_reward',False) else env_params.get('gamma',0.99)\n",
        "        self.logging_Q = {}\n",
        "        self.zero_var_start = zero_var_start\n",
        "        self.no_early_stopping = (env_params.get('circle_mdp', False) or (not env_params.get('discounted_reward', False)) )\n",
        "        self.env_params = env_params\n",
        "        self.env_type = env_type\n",
        "\n",
        "    def train(self):\n",
        "        self.rList = []\n",
        "        self.rList_stepwise = []\n",
        "        self.logging_Q[0] = self.Q.copy()\n",
        "        for k in tqdm(range(1, self.K)):\n",
        "            s = self.env.reset()\n",
        "            d = False\n",
        "            rEpisode = 0\n",
        "            trajectory = {}\n",
        "            r_k = 0\n",
        "            for h in range(self.H):\n",
        "                a = np.random.choice(np.where(self.Q[h,s,:] == self.Q[h,s,:].max())[0])\n",
        "                self.N[h,s,a] += 1\n",
        "                s1, r, d, _ = self.env.step(a)\n",
        "                if self.env_type == 'FrozenLake' and d and self.env_params.get('circle_mdp', False) and s == self.S-1:\n",
        "                  s1 = self.env.reset()\n",
        "                if self.env_type == 'FrozenLake'  and d and not self.env_params.get('discounted_reward', False) and  s == s1 == self.S-1:\n",
        "                  r = self.rList_stepwise[-1]\n",
        "                if d and self.env_params.get('discounted_reward', False) and s1 == self.S-1:\n",
        "                  r = (self.H - h) / self.H\n",
        "\n",
        "                self.D[h][s][a][0] += r\n",
        "                self.D[h][s][a][int(s1) + 1] += 1\n",
        "                trajectory[h] = (s,a,s1,r,d)\n",
        "                r_k += r*(self.gamma**h)\n",
        "                self.rList_stepwise.append(r*(self.gamma**h))\n",
        "                if d and not self.no_early_stopping:\n",
        "                  break\n",
        "                s = s1\n",
        "            if k%10000 == 0:\n",
        "                print([trajectory[h][0] for h in range(self.H) if h in trajectory])\n",
        "            # if self.env_params.get('discounted_reward', False):\n",
        "            #     for h in range(self.H-1, -1, -1):\n",
        "            #         if h in trajectory:\n",
        "            #             s,a,s1,r,d = trajectory[h]\n",
        "            #             PV_next = (self.D[h][s][a][1:] @ self.V[h+1]) / (self.N[h][s][a] + 1)\n",
        "            #             self.Q[h][s][a] = PV_next + self.D[h][s][a][0]  / (self.N[h][s][a] + 1) + np.random.randn() * np.sqrt(self.beta[k] / (self.N[h][s][a] + 1))\n",
        "            #             self.V[h][s] = np.max(self.Q[h][s])\n",
        "            # else:\n",
        "            for h in range(self.H-1, -1, -1):\n",
        "                PV_next = (self.D[h,:,:,1:] @ self.V[h+1] ) / (self.N[h] + 1)\n",
        "                self.Q[h] = PV_next + self.D[h,:,:,0] /  (self.N[h] + 1) + np.random.randn(self.S, self.A) * np.sqrt(self.beta[k] / (self.N[h] + 1))\n",
        "                if self.env_params.get('discounted_reward', False):\n",
        "                    for s in self.Env.absorbing_states:\n",
        "                        self.Q[h, int(s),:] = np.zeros(self.A)\n",
        "                self.V[h] = np.clip(np.max(self.Q[h],axis=1), 0 , (self.H - h)/self.H)\n",
        "\n",
        "            logging_Q = self.Q.copy()\n",
        "            # self.logging_Q[k] = logging_Q\n",
        "            self.rList.append(r_k)\n",
        "        self.trajectory = trajectory\n",
        "\n",
        "    def stepwise_regret(self):\n",
        "        self.regret_step_wise = []\n",
        "        self.Env.compute_transition()\n",
        "        self.Env.value_iteration()\n",
        "        self.pi_star = self.Env.Q_prime.argmax(axis=2)\n",
        "        total_steps = len(self.rList_stepwise)\n",
        "        current_steps = 0\n",
        "        current_cum_regret = 0\n",
        "        while current_steps < total_steps:\n",
        "            s = self.env.reset()\n",
        "            # d = False\n",
        "            for h in range(self.H):\n",
        "                a = int(self.pi_star[h][s])\n",
        "                s1, r, d, _ = self.env.step(a)\n",
        "                if d and self.env_params.get('discounted_reward', False) and s1 == self.S-1:\n",
        "                    r = (self.H - h) / self.H\n",
        "                current_cum_regret += r*(self.gamma**h) - self.rList_stepwise[current_steps]\n",
        "                if current_steps % self.H == 0:\n",
        "                    self.regret_step_wise.append(current_cum_regret)\n",
        "                s = s1\n",
        "                current_steps += 1\n",
        "                if d and not self.no_early_stopping:\n",
        "                    break\n",
        "                if current_steps >= total_steps:\n",
        "                    break\n",
        "\n",
        "    def regret_compute(self):\n",
        "        regret = []\n",
        "        self.Env.compute_transition()\n",
        "        self.Env.value_iteration()\n",
        "        V_prime = self.Env.V_prime\n",
        "        for k in tqdm(range(self.K)):\n",
        "            Q_k = self.logging_Q[k]\n",
        "            policy_k = Q_k.argmax(axis=2)\n",
        "            eval = self.Env.policy_evaluation(policy_k)\n",
        "            if regret:\n",
        "                regret.append(regret[k-1]+ (V_prime[0][0] - eval[0][0]))\n",
        "            else:\n",
        "                regret.append( (V_prime[0][0] - eval[0][0]))\n",
        "\n",
        "        return regret"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSVI_UCB(object):\n",
        "    def __init__(self, H=25, K=20000, c=0.01, env_type = 'FrozenLake', env_params={}):\n",
        "        self.Env = Env_mapping[env_type](H=H, env_params=env_params)\n",
        "        self.env_type = env_type\n",
        "        self.env = self.Env.base_env\n",
        "        self.A = self.Env.A\n",
        "        self.S = self.Env.S\n",
        "        self.H = self.Env.H\n",
        "        self.d = self.S * self.A\n",
        "        self.K = K\n",
        "        self.T = self.K * self.H\n",
        "        self.max_V = self.H if env_params.get('penalty_case',False) else 1\n",
        "        self.c = c # need more tuning\n",
        "        self.p = .05 # need more tuning\n",
        "        self.tau = np.log(2 * self.d  *self.T / self.p)\n",
        "        self.gamma = 1 if not env_params.get('discounted_reward',False) else env_params.get('gamma',0.99)\n",
        "        self.env_params = env_params\n",
        "        self.no_early_stopping = (env_params.get('circle_mdp', False) or (not env_params.get('discounted_reward', False)) )\n",
        "        self.lamb = 1\n",
        "        self.beta = self.c * self.max_V * np.sqrt(self.tau) # * self.d  do we need this d?\n",
        "\n",
        "    def train(self):\n",
        "        self.N = np.ones([self.H, self.S, self.A]) * self.lamb\n",
        "        self.D = np.zeros([self.H, self.S, self.A, self.S+1])\n",
        "        self.Q = np.ones([self.H, self.S, self.A]) * self.max_V\n",
        "        self.V = np.ones([self.H + 1, self.S]) * self.max_V\n",
        "        self.V[self.H] = np.zeros(self.S) # This one is the must\n",
        "        if self.env_params.get('discounted_reward', False):\n",
        "            for s in self.Env.absorbing_states:\n",
        "                self.V[:, int(s)] = np.zeros(self.H + 1)\n",
        "\n",
        "        self.rList = []\n",
        "        # self.rList_stepwise = []\n",
        "        for k in tqdm(range(self.K)):\n",
        "            s = self.env.reset()\n",
        "            d = False\n",
        "            r_k = 0\n",
        "            trajectory = {}\n",
        "            for h in range(self.H):\n",
        "                a = np.random.choice(np.where(self.Q[h][s] == self.Q[h][s].max())[0])\n",
        "                s1, r, d, _ = self.env.step(a)\n",
        "                if self.env_type == 'FrozenLake' and d and self.env_params.get('circle_mdp', False) and s == self.S-1:\n",
        "                  s1 = self.env.reset()\n",
        "                if self.env_type == 'FrozenLake'  and d and not self.env_params.get('discounted_reward', False) and  s == s1 == self.S-1:\n",
        "                  r = self.rList_stepwise[-1]\n",
        "                if d and self.env_params.get('discounted_reward', False) and s1 == self.S-1:\n",
        "                  r = (self.H - h) / self.H\n",
        "                trajectory[h] = (s,a,s1,r,d)\n",
        "                self.N[h,s,a] += 1\n",
        "                self.D[h][s][a][0] += r\n",
        "                self.D[h][s][a][s1 + 1] += 1\n",
        "\n",
        "                r_k += r*(self.gamma**h)\n",
        "                # self.rList_stepwise.append(r*(self.gamma**h))\n",
        "                if d and not self.no_early_stopping:\n",
        "                  break\n",
        "                s = s1\n",
        "\n",
        "\n",
        "            if k%10000 == 0:\n",
        "                print([trajectory[h][0] for h in range(self.H) if h in trajectory])\n",
        "            for h in range(self.H-1, -1, -1):\n",
        "                r_PV_next = (self.D[h,:,:,1:] @ self.V[h+1] ) + self.D[h,:,:,0]\n",
        "                self.Q[h] = np.minimum(r_PV_next/ self.N[h] + self.beta * np.sqrt(1/self.N[h]), self.max_V)\n",
        "                if self.env_params.get('discounted_reward', False):\n",
        "                    for s in self.Env.absorbing_states:\n",
        "                        self.Q[h, int(s),:] = np.zeros(self.A)\n",
        "                self.V[h] = np.clip(np.max(self.Q[h],axis=1), 0 , (self.H - h)/self.H)\n",
        "\n",
        "            self.rList.append(r_k)"
      ],
      "metadata": {
        "id": "j5i5XVLaLIwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHTcGYy95bIt"
      },
      "outputs": [],
      "source": [
        "def test_on_params(H=15,K=20000,c=0.01,learning_type='Hoeffding',env_type='FrozenLake',env_params={},eval_V_pi = False,experiment_name_suffix=\"\"):\n",
        "    env_p = env_params.get('drift_probability',0.85) if env_type == 'DrunkMan' else 0.33\n",
        "    absorbing_state = 'No' if env_params.get('simple_example', True) else 'Many'\n",
        "    penalty_case = env_params.get('penalty_case',False)\n",
        "    gamma = 1 if not env_params.get('discounted_reward',False) else env_params.get('gamma',0.99)\n",
        "    respawn = env_params.get('circle_mdp',False)\n",
        "    state_space = env_params.get('space_length', 7) if env_type=='DrunkMan' else  len(env_params.get('desc_map', 4))**2\n",
        "    wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"PEQL_1128_F\",\n",
        "    name = f'{learning_type},env:{env_type},env_p={env_p},penalty_case:{penalty_case},state_space_size:{state_space}',\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"H\": H,\n",
        "    \"Algo_name\" : learning_type,\n",
        "    \"c\":c,\n",
        "    \"b\":1,\n",
        "    \"env_p\": env_params.get('drift_probability',0.85) if env_type == 'DrunkMan' else 0.33,\n",
        "    'p':0.05,\n",
        "    'K':K,\n",
        "    \"lr_type\": \"adaptive\",\n",
        "    \"Env_name\": env_type,\n",
        "    'penalty_case':env_params.get('penalty_case',False),\n",
        "    'env_no_absorbing_state':env_params.get('simple_example', True),\n",
        "    'discounted_reward':env_params.get('discounted_reward',False),\n",
        "    'gamma':1 if not env_params.get('discounted_reward',False) else env_params.get('gamma',0.99)\n",
        "    }\n",
        ")\n",
        "    if learning_type == 'Hoeffding':\n",
        "        Q_learning = ProvablyEfficientQLearning(H=H, K=K, c=np.sqrt(c),env_type=env_type,env_params=env_params)\n",
        "    elif learning_type == 'Posterior Sampling':\n",
        "        Q_learning = PosteriorSamplingQLearning(H=H,K=K,c=c,env_type=env_type,env_params=env_params)\n",
        "    elif learning_type == 'RLSVI':\n",
        "        Q_learning = RLSVI(H=H,K=K,c=c,env_type=env_type,env_params=env_params)\n",
        "    elif learning_type == 'LSVI_UCB':\n",
        "        Q_learning = LSVI_UCB(H=H,K=K,c=np.sqrt(c),env_type=env_type,env_params=env_params)\n",
        "    Q_learning.train()\n",
        "    Q_learning.Env.compute_transition()\n",
        "    Q_learning.Env.value_iteration()\n",
        "    V_prime = Q_learning.Env.V_prime\n",
        "    rList = Q_learning.rList\n",
        "    cum_regret = 0\n",
        "    regret_list = []\n",
        "    cum_reward = 0\n",
        "\n",
        "    if eval_V_pi:\n",
        "        regret = Q_learning.regret_compute()\n",
        "\n",
        "    for idx in range(len(rList)):\n",
        "        i = rList[idx]\n",
        "        cum_regret += V_prime[0][0] - i\n",
        "        regret_list.append(cum_regret)\n",
        "        cum_reward += i\n",
        "        if not eval_V_pi:\n",
        "          wandb.log({'cumulative_regret':cum_regret,'current_episode_regret':V_prime[0][0] - i,'reward':cum_reward })#, 'Q_left':Q_learning.logging_Q[idx][0,0,0], 'Q_right':Q_learning.logging_Q[idx][0,0,1],\n",
        "                   #'Q_left_last':Q_learning.logging_Q[idx][-2,-2,0], 'Q_right_last':Q_learning.logging_Q[idx][-2,-2,1]})\n",
        "        else:\n",
        "          wandb.log({'cumulative_regret':regret[idx],'cumulative_regret_with_reward':cum_regret,'current_episode_regret':V_prime[0][0] - i,'reward':cum_reward, 'Q_left':Q_learning.logging_Q[idx][0,0,0], 'Q_right':Q_learning.logging_Q[idx][0,0,1],\n",
        "                   'Q_left_last':Q_learning.logging_Q[idx][-2,-2,0], 'Q_right_last':Q_learning.logging_Q[idx][-2,-2,1]})\n",
        "    # if not Q_learning.no_early_stopping:\n",
        "    #     Q_learning.stepwise_regret()\n",
        "    #     stepwise_regret_list = Q_learning.regret_step_wise\n",
        "    #     for idx in range(len(stepwise_regret_list)):\n",
        "    #         wandb.log({'cumulative_stepwise_regret':stepwise_regret_list[idx]})\n",
        "    return regret_list"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}